import streamlit as st
import pandas as pd
import chardet
from io import StringIO, BytesIO

# --- Configuration and Title ---
# st.set_page_config(layout="wide")
st.title("ğŸ“„ CSV æ–‡ä»¶åˆå¹¶")

# Define the maximum number of bytes to read for encoding detection
CHUNK_SIZE_FOR_DETECTION = 100000 
COMMON_CHINESE_ENCODING = 'gb18030' 

# --- File Uploader ---
uploaded_files = st.file_uploader(
    "ğŸ“‚ ä¸Šä¼ æ•°æ® (CSV æ ¼å¼)", 
    type="csv", 
    accept_multiple_files=True
)

if not uploaded_files:
    st.info("â¬†ï¸ è¯·ä¸Šä¼  CSV æ–‡ä»¶ä»¥å¼€å§‹åˆå¹¶ã€‚")
    st.stop()

# --- Core Logic Functions ---

@st.cache_data(show_spinner=False)
def process_and_merge_files(uploaded_files):
    """
    Detects encoding for each file, reads it into a DataFrame using fallbacks,
    and then concatenates all successfully read DataFrames.
    """
    all_dfs = []
    log_messages = []
    
    # 1. Iteration and Processing
    for file in uploaded_files:
        file_name = file.name
        log_messages.append(f"--- æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name} ---")
        
        # Read the file content as bytes for chardet
        bytes_data = file.read() 
        
        # 1.1 Detect Encoding using chardet
        detected_encoding = 'utf-8' # Default fallback
        
        try:
            # Analyze the raw bytes
            result = chardet.detect(bytes_data[:CHUNK_SIZE_FOR_DETECTION]) # Detect on a sample chunk
            
            if result['confidence'] > 0.7:
                detected_encoding = result['encoding'].lower()
        
        except Exception as e:
            log_messages.append(f"    - âš ï¸ è­¦å‘Š: æœªçŸ¥ç¼–ç ï¼Œä½¿ç”¨é»˜è®¤ UTF-8ã€‚é”™è¯¯: {e}")
            detected_encoding = 'utf-8'

        
        # 1.2 Read the CSV using the detected/fallback encoding
        df = None
        
        # Attempt 1: Detected/Default encoding
        try:
            # We use StringIO to wrap the decoded bytes data for pandas
            df = pd.read_csv(BytesIO(bytes_data), encoding=detected_encoding)
            # log_messages.append(f"    - âœ… æˆåŠŸè¯»å– (ä½¿ç”¨: {detected_encoding})")

        except UnicodeDecodeError:
            # Attempt 2: Try the robust Chinese encoding 'gb18030'
            try:
                df = pd.read_csv(BytesIO(bytes_data), encoding=COMMON_CHINESE_ENCODING)
                # log_messages.append(f"    - âœ… æˆåŠŸè¯»å– (ä½¿ç”¨ GB18030 åå¤‡æ–¹æ¡ˆ)")

            except UnicodeDecodeError:
                # Attempt 3: Try 'latin1' for non-Unicode/European files
                try:
                    df = pd.read_csv(BytesIO(bytes_data), encoding='latin1')
                    # log_messages.append(f"    - âœ… æˆåŠŸè¯»å– (ä½¿ç”¨ Latin1 åå¤‡æ–¹æ¡ˆ)")
                
                except Exception as e:
                    # Final failure handling
                    log_messages.append(f"    - âŒ è¯»å–å¤±è´¥:æ–‡ä»¶å·²è·³è¿‡ã€‚é”™è¯¯: {e}")
                    continue # Skip to the next file
        
        except Exception as e:
            # Handle other pd.read_csv errors (e.g., incorrect separator, empty file)
            log_messages.append(f"    - âŒ ç»“æ„é”™è¯¯: æ–‡ä»¶è¯»å–å¤±è´¥ã€‚æ–‡ä»¶å·²è·³è¿‡ã€‚é”™è¯¯: {e}")
            continue

        # Append the successfully read DataFrame
        if df is not None:
            all_dfs.append(df)
            
    
    # 2. Concatenation
    if not all_dfs:
        return None, log_messages

    try:
        big_df = pd.concat(all_dfs, ignore_index=True)
        log_messages.append("\n--- åˆå¹¶ç»“æœ ---")
        log_messages.append(f"ğŸ‰ æˆåŠŸåˆå¹¶ {len(all_dfs)} ä¸ªæ–‡ä»¶ã€‚")
        log_messages.append(f"æ€»è¡Œæ•°: {len(big_df)} | æ€»åˆ—æ•°: {len(big_df.columns)}")
        return big_df, log_messages
        
    except Exception as e:
        log_messages.append(f"\nâŒ åˆå¹¶å¤±è´¥ã€‚è¯·æ£€æŸ¥æ–‡ä»¶åˆ—åæ˜¯å¦å®Œå…¨ä¸€è‡´ã€‚é”™è¯¯: {e}")
        return None, log_messages


# --- Streamlit UI Rendering ---

# The processing function runs when the button is clicked
if st.button("ğŸš€ åˆå¹¶", type="primary"):
    with st.spinner("æ­£åœ¨åˆå¹¶å’Œå¤„ç†æ–‡ä»¶..."):
        # The function handles all reading, encoding, and merging
        final_df, logs = process_and_merge_files(uploaded_files)
    
    st.subheader("ğŸ› ï¸ å¤„ç†æ—¥å¿—")
    st.code("\n".join(logs), language="text")

    if final_df is not None:
        st.success("âœ… æ–‡ä»¶åˆå¹¶æˆåŠŸï¼")

        # Display Summary
        c1, c2 = st.columns(2)
        c1.metric("åˆå¹¶åçš„æ€»è¡Œæ•° (Total Rows)", f"{len(final_df):,}")
        c2.metric("åˆå¹¶åçš„æ€»åˆ—æ•° (Total Columns)", f"{len(final_df.columns)}")

        st.subheader("ğŸ“Š åˆå¹¶ç»“æœé¢„è§ˆ (å‰ 5 è¡Œ)")
        st.dataframe(final_df.head(), use_container_width=True)

        # Download Button
        # Create a buffer for the combined CSV file
        csv_buffer = BytesIO()
        # Save to buffer using UTF-8 for universal compatibility
        final_df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_buffer.seek(0)

        st.download_button(
            label="â¬‡ï¸ ä¸‹è½½åˆå¹¶åçš„ CSV æ–‡ä»¶ (UTF-8)",
            data=csv_buffer,
            file_name="combined_data.csv",
            mime="text/csv"
        )
    else:
        st.error("âŒ åˆå¹¶å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚")